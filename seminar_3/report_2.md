# Отчет 2. Исследование метода Q-learning в среде Frozen Lake 

## 1. Сравнение алгоритмов V и Q learning (2 балла)
Для алгоритма `V learning` на поле (4х4) при `gamma=0.9` сходимость достигается за ~30 итерации (min = 16, max = 46). 
Графики зависимости reward от количества итераций приведены ниже. 
![Screenshot_3.png](images%2FScreenshot_3.png)

Для алгоритма `Q learning` на поле (4х4) при `gamma=0.9` сходимость достигается за ~28 итерации (min = 15, max = 62). 
Графики зависимости reward от количества итераций приведены ниже. 
![Screenshot_2.png](images%2FScreenshot_2.png)

**Вывод:** В конкретно данной задаче нельзя явно указать алгоритм, который сходится быстрее, виден легкий перевес в пользу Q-learning, но незначительный.
Разница между алгоритмами в том, что в Q-learning целью является ценность дейтсвия, то есть полезность выполнения какого-то действия а в состоянии S,
а в V-learning оценивается непосредственно полезность нахождения в состоянии s.


## 2. Влияние гиперпараметра `GAMMA` на скорость сходимости . (2 балла)

Для алгоритма `Q learning` и 'V learning' на поле (4х4) проведем испытания при `gamma=0.6`
Графики зависимости reward от количества итераций для Q learning. 
![Screenshot_add_2.png](images%2FScreenshot_add_2.png)
среднее количество итераций до сходимости = 2611 (max = 6717, min = 91)

Графики зависимости reward от количества итераций для V learning. 
![Screenshot_add_1.png](images%2FScreenshot_add_1.png)
среднее количество итераций до сходимости = 3897 (max = 11395, min = 772)

Для алгоритма `Q learning` и 'V learning' на поле (4х4) проведем испытания при `gamma=0.99`
Графики зависимости reward от количества итераций для Q learning. 
![Screenshot_add_4.png](images%2FScreenshot_add_4.png)
среднее количество итераций до сходимости = 43.5 (max = 92, min = 19)

Графики зависимости reward от количества итераций для V learning.
![Screenshot_add_3.png](images%2FScreenshot_add_3.png)
среднее количество итераций до сходимости = 42 (max = 70, min = 21)

**Вывод:** Увеличение гиперпараметра `GAMMA` приводит к около таким же результатам. Это связано с тем, что сама задача не особо трудозатратная и нельзя сказать, насколько чувствуется разница в гиперпараметрах, тем более, что значение, которое мы выбрали 0.99 слегка больше изначального 0.9, важно понимать, что дисконт фактор имеет чем ближе к 1, тем будущие награды имеют такую же важность, как и текущие

Уменьшение гиперпараметра `GAMMA` приводит к замедлению скорости сходимости. Это связано с тем, что чем ближе дисконт фактор к 0, тем более весомыми становятся награды в моменте, что в определнных ситуациях могут тормозить агента и количество шагов до сходимости может увеличится.

## 3. Сравнение алгоритмов V и Q learning на поле большего размера (3 балла)
Возьмем модель frozenlake с размером поля 8*8 и обучим с такими же гиперпараметрами.
Чтобы было удобнее, сначала вставим получившееся число итераций со старым, маленьким полем, а потом добавим с новым
![Screenshot_2.png](images%2FScreenshot_2.png)
![Screenshot_3.png](images%2FScreenshot_3.png)

Теперь вставим то, что получилось на новом поле:
![Screenshot_4.png](images%2FScreenshot_4.png)
![Screenshot_5.png](images%2FScreenshot_5.png)

Мы можем видеть, что число итераций увеличилось. Напомним, что:
Для алгоритма `Q learning` на поле (4х4) при `gamma=0.9` сходимость достигается за ~28 итерации (min = 15, max = 62). 
Для алгоритма `V learning` на поле (4х4) при `gamma=0.9` сходимость достигается за ~30 итерации (min = 16, max = 46). 
В случае поля 8*8 получаем:
Для алгоритма `Q learning`: сходимость достигается за ~853 итерации (min = 148, max = 2174).
Для алгоритма `V learning`: сходимость достигается за ~366 итерации (min = 30, max = 1203).

Почему так получилось?
Увеличение размера среды обычно увеличивает сложность задачи. В случае FrozenLake означает больше препятствий и более длинные пути к цели. Это делает задачу более сложной для агента.
Увеличение размера среды может привести к увеличению времени, необходимого для обучения агента. Более сложная среда требует более глубокого и длительного исследования, чтобы агент мог научиться эффективно действовать в ней.

Однако мы видим, что алгоритм V learning справляется быстрее.

