# Отчет 2. Исследование метода Q-learning в среде Frozen Lake 

## 1. Сравнение алгоритмов V и Q learning (2 балла)
Для алгоритма `V learning` на поле (4х4) при `gamma=0.9` сходимость (mean reward > 0.85) достигается в среднем за 53 итерации (от 30 до 85). 
Графики зависимости reward от количества итераций приведены ниже. 



Для алгоритма `Q learning` на поле (4х4) при `gamma=0.9` сходимость (mean reward > 0.85) достигается в среднем за 53 итерации (от 30 до 85). 
Графики зависимости reward от количества итераций приведены ниже. 



**Вывод:** Алгоритм обучения ценности состояний более эффективен чем обучение ценности действий. Это связано с тем, что обучение ценности действий оценивает каждое действие для каждого состояния, что делает данный алгоритм более трудозатратным по времени обучения.


## 2. Влияние гиперпараметра `GAMMA` на скорость сходимости . (2 балла)

Для алгоритма `Q learning` на поле (4х4) при `gamma=0.8` сходимость (mean reward > 0.85) достигается в среднем за 53 итерации (от 30 до 85). 
Графики зависимости reward от количества итераций приведены ниже. 

<img src="imgs/reward_1.png"/>

**Вывод:** Увеличение гиперпараметра `GAMMA` приводит к ... Это связано с тем, что ...   

Уменьшение гиперпараметра `GAMMA` приводит к ... Это связано с тем, что ... 

## 3. Сравнение алгоритмов V и Q learning на поле большего размера (3 балла)

