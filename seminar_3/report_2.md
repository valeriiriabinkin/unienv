# Отчет 2. Исследование метода Q-learning в среде Frozen Lake 

## 1. Сравнение алгоритмов V и Q learning (2 балла)
Для алгоритма `V learning` на поле (4х4) при `gamma=0.9` сходимость (mean reward > 0.85) достигается в среднем за 53 итерации (от 30 до 200). 
Графики зависимости reward от количества итераций приведены ниже. 
![Screenshot_2](https://github.com/valeriiriabinkin/unienv/assets/45901469/e8386b89-c642-42e8-8cc9-e0ee5b05d7c4)


Для алгоритма `Q learning` на поле (4х4) при `gamma=0.9` сходимость (mean reward > 0.85) достигается в среднем за 53 итерации (от 30 до 200). 
Графики зависимости reward от количества итераций приведены ниже. 
![Screenshot_1](https://github.com/valeriiriabinkin/unienv/assets/45901469/5fac658e-c021-4625-ad2d-7ebba349e8d9)



**Вывод:** Алгоритм обучения ценности состояний более эффективен чем обучение ценности действий. Это связано с тем, что обучение ценности действий оценивает каждое действие для каждого состояния, что делает данный алгоритм более трудозатратным по времени обучения.


## 2. Влияние гиперпараметра `GAMMA` на скорость сходимости . (2 балла)

Для алгоритма `Q learning` на поле (4х4) при `gamma=0.6` сходимость (mean reward > 0.85) достигается в среднем за более чем 1000 итераций
Графики зависимости reward от количества итераций приведены ниже. 
![Screenshot_3](https://github.com/valeriiriabinkin/unienv/assets/45901469/7da90545-f985-4bac-aafa-1fe404fc0885)

![Screenshot_4](https://github.com/valeriiriabinkin/unienv/assets/45901469/752f3ae9-378e-46b9-bd34-f1dbd7b2de17)


**Вывод:** Увеличение гиперпараметра `GAMMA` приводит к ускорению скорости сходимости. Это связано с тем, что если gamma    ближе к 1, то награда в будущем имеет больше вес, чем награда в моменте. будущие вознаграждения более заметно влияют на обновление ценностей состояний или действий, что приводит к более быстрой сходимости

Уменьшение гиперпараметра `GAMMA` приводит к замедлению скорости сходимости. Это связано с тем, что как говорилось выше, чем дальше от 1, тем более весомыми становятся награды в моменте, что не так сильно влияют на обновление ценностей состояния или действий

## 3. Сравнение алгоритмов V и Q learning на поле большего размера (3 балла)
Увеличение поля, то есть изменение размерности с 4*4 по умолчанию на имеющийся в библиотеке Gym. импорт большего поля может быть как 8*8, то есть 64 клетки, 16*16, то есть 256 клетки. Увеличение поля означает увеличение нейронов необученных, что влечет за собой более долгое обучение к желаемой награде, так как сходится градиент будет дольше.
![Screenshot_5](https://github.com/valeriiriabinkin/unienv/assets/45901469/c4d1166d-36fb-4ed0-bf8c-dd7fd62c32f1)


![Screenshot_6](https://github.com/valeriiriabinkin/unienv/assets/45901469/e937a53c-b26c-4f4c-abc7-069e55cca85f)

