# Задание 1
Изучите алгоритм табличного обучения (tabular Q learning) в среде Frozen Lake. Chapter06/01_frozenlake_q_learning.py. Реализуйте случайный выбор действия при нулевой ценности. Исследуйте влияние гиперпараметра альфа на среднее количество шагов обучения (>=5 повторений).
Для того, чтобы реализовать случайный выбор действия при нулевой ценности, написал следующие строчки кода в функцию play_episode
            


if q_value == 0:
    action = env.action_space.sample()

Таким образом мы избавимся от простаивания в начале обучения, когда у нас долгое время идет значение 0.
Далее рассмотрим влияние параметра alpha на среднее количество шагов обучения.
Взятые параметры alpha: 0.2, 0.5, 0.9

Результаты с alpha = 0.2:
![alpha0_2_q_learning-determined.jpg](..%2F..%2F..%2FOneDrive%2FDocuments%2Fimages%2Falpha0_2_q_learning-determined.jpg)

Результаты с alpha = 0.5:
![alpha0_5_q_learning-determined.jpg](..%2F..%2F..%2FOneDrive%2FDocuments%2Fimages%2Falpha0_5_q_learning-determined.jpg)

Результаты с alpha = 0.9:
![alpha0_9_q_learning-determined.jpg](..%2F..%2F..%2FOneDrive%2FDocuments%2Fimages%2Falpha0_9_q_learning-determined.jpg)

Среднее количество шагов при alpha = 0.2: 8629 за 5 повторных запусков
Среднее количество шагов при alpha = 0.5: 6960.4 за 5 повторных запусков
Среднее количество шагов при alpha = 0.9: 17868 за 5 повторных запусков

Обычно при высоком значении ALPHA новые обновления значений Q будут сильно зависеть от недавних наблюдений. Это может привести к тому, что агент будет быстро адаптироваться к новым данным, но при этом может страдать от нестабильности и сильных колебаний в обучении.
и при низком значении ALPHA новые обновления значений Q будут медленно адаптироваться к новым данным, так как предыдущие оценки будут иметь больший вес. Это обычно приводит к более стабильному, но медленному обучению.
Но, как мы видим, в нашем случае лушим выбором стало среднее значение 0.5. В нашем случае высокое значение alpha привело к тому, что агент
слишком быстро  адаптировался ко новым данным и это привело к нестабильному поведению и затруднению сходимости.

# Задание 2
Чтобы обучить данную модель, нужно было активировать видеокарту (GPU) для улучшения скорости обучения.
использовалась данная рабочая машина: ryzen 5 5600x, rtx 3060ti, 32gb ddr4. Обучение происходило в jupyter notebook.
Прирост в сравнении обучения без видеокарты был существенен: 95 f/s в среднем против 29 f/s до использования видеокарты.
Лучшая модель называется Pong-v5-best_19.dat. Видео, полученное с помощью 03_dqn_play.py, лежит в папке video.

#Задание 3
Изменять я буду гиперпараметр GAMMA, который указывает на то, какую долю будущих вознаграждений мы учитываем при вычислении ожидаемых суммарных вознаграждений.
Выбранные значения GAMMA я выбрал 0.5 и 0.2.