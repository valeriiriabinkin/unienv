# Задание 2 (Изучите алгоритм REINFORCE Chapter11/
02_cartpole_reinforce.py. Исследуйте влияние гиперпараметров на среднее количество шагов сходимости.)
Во втором задании мы знакомимся с реализацией алгоритма REINFORCE в среде cartpole.
У нас представлены следующие параметры:Гиперпараметры: Определяются гиперпараметры алгоритма, такие как коэффициент дисконтирования GAMMA, скорость обучения LEARNING_RATE и количество эпизодов для обучения EPISODES_TO_TRAIN.
Для начала выясним, что в теории влечет за собой изменение тех или иных гиперпараметров:
GAMMA (коэффициент дисконтирования):

Увеличение GAMMA приведет к увеличению влияния будущих вознаграждений на текущие действия. Это может увеличить стабильность обучения и способствовать более долгосрочному планированию агента.
Уменьшение GAMMA приведет к уменьшению влияния будущих вознаграждений на текущие действия. Это может привести к более короткосрочному планированию агента и более чувствительному к непосредственным результатам.

LEARNING_RATE (скорость обучения):

Увеличение LEARNING_RATE может ускорить сходимость, особенно в начале обучения. Однако слишком высокая скорость обучения может привести к осцилляциям или расхождению.
Уменьшение LEARNING_RATE может увеличить стабильность обучения и сделать процесс более надежным, но может потребоваться больше времени для сходимости.

EPISODES_TO_TRAIN (количество эпизодов для обучения):

Увеличение EPISODES_TO_TRAIN может привести к более стабильному и точному обучению, так как агенту предоставляется больше данных для обучения.
Однако это также может увеличить время обучения и требования к вычислительным ресурсам.

У нас будет 3 испытания, в каждом из которов 5 повторений: запуск на стандартных параметрах, запуск с lr = 0.005
и запуск с lr = 0.005 и episodes_to_train = 16.

В наш SummaryWriter записывается:
reward: Записывается награда (reward), полученная агентом в каждом шаге обучения. Это позволяет отслеживать изменение награды во времени и оценить процесс обучения.

reward_100: Записывается среднее значение награды за последние 100 эпизодов. Это помогает оценить стабильность обучения и его прогресс на протяжении времени.

episodes: Записывается количество завершенных эпизодов на каждом шаге обучения. Это помогает отслеживать, сколько эпизодов агент завершил к моменту текущего шага.

Реузльтаты со стандартными гиперпараметрами:
![Screenshot_1.jpg](imgs_sem_6%2FScreenshot_1.jpg)
![Screenshot_2.jpg](imgs_sem_6%2FScreenshot_2.jpg)
![Screenshot_3.jpg](imgs_sem_6%2FScreenshot_3.jpg)

Реузльтаты с lr = 0.005
![Screenshot_4.jpg](imgs_sem_6%2FScreenshot_4.jpg)
![Screenshot_5.jpg](imgs_sem_6%2FScreenshot_5.jpg)
![Screenshot_6.jpg](imgs_sem_6%2FScreenshot_6.jpg)

Реузльтаты с lr = 0.005 и episodes_to_train = 16
![Screenshot_7.jpg](imgs_sem_6%2FScreenshot_7.jpg)
![Screenshot_8.jpg](imgs_sem_6%2FScreenshot_8.jpg)
![Screenshot_9.jpg](imgs_sem_6%2FScreenshot_9.jpg)

в первом испытании среднее количество шагов, при котором алгоритм завершился, составил ~32133
во втором испытании уже среднее количество шагов составило ~35955
в третьем составило ~54047

Вывод: в рамках поставленного порогового значения остановки, стандартные параметры самые отптимальные,
однако в случае, если значение остановки будет в разы больше, полагаю, уже будут лучше показывать себя с другими значениями шиперпараметров, приблеженные к испытаниям 2 и 3.

# Задание 3
